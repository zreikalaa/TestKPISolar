# Test Technique â€“ Traitement de DonnÃ©es Ã‰nergÃ©tiques avec PySpark

## ğŸ“ Fichiers fournis

- `inverter_yields.csv` : Mesures toutes les 10 minutes par onduleur
- `static_inverter_info.csv` : MÃ©tadonnÃ©es des onduleurs
- `sldc_events.csv` : Ã‰vÃ©nements sur les Ã©quipements (IEC category)
- `site_median_reference.csv` : Valeurs de rendement spÃ©cifiques de rÃ©fÃ©rence par site

## ğŸ¯ Objectif

Construire un pipeline PySpark local qui :

1. Lit les 4 jeux de donnÃ©es CSV
2. Joints `inverter_yields` avec :
   - `static_inverter_info` via `logical_device_mrid`
   - `sldc_events` en chevauchement temporel
   - `site_median_reference` sur `project_code` et `ts_start`
3. Calcule `potential_production = specific_yield_ac Ã— ac_max_power Ã— 1/6` (10min en heures)
4. Ne conserve que les inverters `"PV"` qui **ne sont pas "AC-Coupled"**
5. Produit un fichier `parquet` partitionnÃ© par `project_code` et `year_month`

## ğŸ“Š Bonus

- Ã‰crire une requÃªte SQL pour les sites sous-performants
- Proposer un schÃ©ma Glue/Athena adaptÃ©
- Ajouter des tests Spark
- Faire en sorte que la Pipeline tourne sur n'importe quel environnement.

## ğŸšš Delivery

CrÃ©er un **fork** de ce projet (**pas de nouvelle branche**) et rajoutez les users [jaumes5](https://github.com/jaumes5) et 
[DieuveilleWebH3](https://github.com/DieuveilleWebH3).

Bon courage !
